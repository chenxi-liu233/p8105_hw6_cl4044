---
title: "p8105_hw6_cl4044"
author: "Chenxi Liu"
date: "12/7/2020"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


```{r load_libraries}
library(tidyverse)
library(modelr)
library(p8105.datasets)
```

### Problem 1

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) %>% 
  filter(
    victim_race %in% c("White", "Black"),
    city_state != "Tulsa, AL") %>% 
  select(city_state, resolution, victim_age, victim_race, victim_sex)
```


Start with one city.

```{r}
baltimore_df =
  homicide_df %>% 
  filter(city_state == "Baltimore, MD")
glm(resolution ~ victim_age + victim_race + victim_sex, 
    data = baltimore_df,
    family = binomial()) %>% 
  broom::tidy() %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(term, OR, starts_with("CI")) %>% 
  knitr::kable(digits = 3)
```


Try this across cities.

```{r}
models_results_df = 
  homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = 
      map(.x = data, ~glm(resolution ~ victim_age + victim_race + victim_sex, data = .x, family = binomial())),
    results = map(models, broom::tidy)
  ) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(city_state, term, OR, starts_with("CI")) 
```

```{r}
models_results_df %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```



## Problem 2

Import and tidy the dataset 
```{r}
baby_df = 
  read_csv("./data/birthweight.csv") %>%
  janitor::clean_names() %>%
  mutate(
    across(
      .cols = c("frace", "mrace"),
      ~ case_when(
        .x == 1 ~ "White",
        .x == 2 ~ "Black",
        .x == 3 ~ "Asian",
        .x == 4 ~ "Puero Rican",
        .x == 8 ~ "Other",
        .x == 9 ~ "Unknown"
        )
    ),
    across(where(is.character), as.factor)
    ) %>%
  mutate(
    babysex = case_when(
      babysex == 1 ~ "male",
      babysex == 2 ~ "female"
      ), 
     malform = case_when(malform == 0~"absent", 
                         malform == 1 ~ "present"),
    across(where(is.character), as.factor)
  )
      
      
```

Stepwise regression model
```{r}
full_model = lm(bwt ~ ., data = baby_df) 
step_model <- MASS::stepAIC(full_model, direction = "both", 
                      trace = FALSE)
step_model %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
```


Plot predicton values vs. residuals

```{r}
baby_df %>% 
  modelr::add_residuals(step_model) %>% 
  modelr::add_predictions(step_model) %>%
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = 0.3) +
  labs(
    title = "Predicton vs. Residual",
    x = "Predicton",
    y = "Residual"
  )
```

According to the plot above, most of the points are clustered around zero. There are some large residual points when prediction value is under 1000. 

Model 2: One using length at birth and gestational age as predictors (main effects only)
```{r}
model_2 = lm(bwt ~ blength + gaweeks, data = baby_df)

model_2 %>%
  broom::tidy() %>% 
  knitr::kable(digits = 3)
  
```

Model 3: One using head circumference, length, sex, and all interactions (including the three-way interaction) between these
```{r}
model_3 = lm(bwt ~ bhead * blength * babysex, data = baby_df) 
model_3 %>%
  broom::tidy() %>% 
  knitr::kable(digits = 3)
```

```{r}
baby_df %>% 
  gather_predictions(step_model, model_2, model_3) %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = blength, y = bwt)) + 
  geom_point(alpha = .5) +
  geom_line(aes(y = pred), color = "red") + 
  facet_grid(~model)
```
Itâ€™s not clear which is best. 

Compare three models in terms of the cross-validated prediction error.

```{r}
cv_df =
  crossv_mc(baby_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_res = 
  cv_df %>%
  mutate(
      my_model = map(train, ~step_model),
      smaller_model = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
      larger_model = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))
  ) %>%
  mutate(
      my_model_rmse = 
        map2_dbl(my_model, test, ~rmse(model = .x, data = .y)),
      smaller_model_rmse = 
        map2_dbl(smaller_model, test, ~rmse(model = .x, data = .y)),
      larger__rmse = 
        map2_dbl(larger_model, test, ~rmse(model = .x, data = .y))
  )

cv_res %>% 
  select(ends_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() +
  labs(
    title = "RMSE for the three Models",
    x = "Models",
    y = "RMSE"
  )
```

## Problem 3

Load the dataset 
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Bootstrap 5000 times
```{r}
bootstrap_results =
  weather_df %>% 
  modelr::bootstrap(n = 50) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results_glance = map(models, broom::glance),
    results_tidy = map(models, broom::tidy)
    ) %>% 
  unnest(results_glance, results_tidy) %>%
  select(id = .id, term, estimate, r.squared)

bootstrap_results %>% 
  group_by(term) %>% 
  summarize(boot_mean = mean(estimate),
            boot_se = sd(estimate),
            r_squared_mean = mean(r.squared),
            r_squared_sd = sd(r.squared)) %>% 
  knitr::kable(digits = 3)
```
The table above showed the mean of estimate, the standard deviation of estimate and mean of rsquared after bootstrap 5000 times. 

Plot the distribution graph of r^2 
```{r}
bootstrap_results %>% 
  ggplot(aes(x = r.squared)) +
  geom_density() +
  labs(
    title = "Distribution of R Squared Hat",
    x = "R Squared Hat",
    y = "Density"
  )
```
The distribution of r^2 is approximately normal with the mean 0.911 and the  standard error of 0.009. 

Plot the distribution graph of log(beta0 hat * beta1 hat).
```{r}
bootstrap_results_log =
  bootstrap_results %>% 
  mutate(term = recode(term, '(Intercept)' = 'intercept')) %>%
  pivot_wider(names_from = term, values_from = estimate) %>%
  mutate(log_coef_intereaction = log(intercept*tmin))

bootstrap_results_log %>%
  ggplot(aes(x = log_coef_intereaction)) +
  geom_density() +
  labs(
    title = "Distribution of log(beta0 hat * beta1 hat)",
    x = "log(beta0 hat * beta1 hat)",
    y = "Density"
  )
bootstrap_results_log %>%
  summarize(intersection_mean = mean(log_coef_intereaction),
            intersection_se = sd(log_coef_intereaction)) %>%
  knitr::kable(digits = 3)
  
```
The distribution of log(beta0 hat * beta1 hat). is approximately normal, with the mean 2.013 and the standard error of 0.024. 

Using the 5000 bootstrap estimates, identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for r^2 and log(beta0 hat * beta1 hat):
```{r}
tibble(
  quantile = c("2.5 %", "97.5 %"),
  'r.squared' = quantile(bootstrap_results_log$r.squared, c(0.025,0.975)),
  'log.muti.beta' = quantile(bootstrap_results_log$log_coef_intereaction, c(0.025,0.975))
) %>%
   knitr::kable(digits = 3)
```
The 95% confidence interval for r^2 is `r  quantile(bootstrap_results_log$r.squared, c(0.025,0.975))`.

The 95% confidence interval for log(beta0 hat * beta1 hat) is (0.8530, 0.8945).
